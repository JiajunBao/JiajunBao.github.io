<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="Two-Stage Object Detector - Faster R-CNN"><meta name="keywords" content="Object Detection,Faster R-CNN"><meta name="author" content="Jiajun (Jayson) Bao"><meta name="copyright" content="Jiajun (Jayson) Bao"><title>Two-Stage Object Detector - Faster R-CNN | Jiajun (Jayson) Bao's</title><link rel="shortcut icon" href="[object Object]"><link rel="stylesheet" href="/css/index.css?version=1.7.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.7.0"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.7.2/css/all.css?version=1.7.0"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.css"><script src="https://cdn.jsdelivr.net/npm/gitalk@latest/dist/gitalk.min.js"></script><script src="https://cdn.jsdelivr.net/npm/blueimp-md5@2.10.0/js/md5.min.js"></script><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script src="https://v1.hitokoto.cn/?encode=js&amp;charset=utf-8&amp;select=.footer_custom_text" defer></script><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  }
} </script><meta name="generator" content="Hexo 4.2.0"></head><body><canvas class="fireworks"></canvas><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="true"><div class="toggle-sidebar-info text-center"><span data-toggle="Toggle article">Toggle site</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Getting-Started"><span class="toc-number">1.</span> <span class="toc-text">Getting Started</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Install-starter-code"><span class="toc-number">1.1.</span> <span class="toc-text">Install starter code</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Setup-code"><span class="toc-number">1.2.</span> <span class="toc-text">Setup code</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Setup-code-1"><span class="toc-number">1.3.</span> <span class="toc-text">Setup code</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Import-functions-from-previous-notebook"><span class="toc-number">1.4.</span> <span class="toc-text">Import functions from previous notebook</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Load-PASCAL-VOC-2007-data"><span class="toc-number">1.5.</span> <span class="toc-text">Load PASCAL VOC 2007 data</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Visualize-PASCAL-VOC-2007"><span class="toc-number">1.6.</span> <span class="toc-text">Visualize PASCAL VOC 2007</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Region-Proposal-Networks-RPN"><span class="toc-number">2.</span> <span class="toc-text">Region Proposal Networks (RPN)</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Anchor"><span class="toc-number">2.1.</span> <span class="toc-text">Anchor</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Activated-positive-and-negative-anchors"><span class="toc-number">2.2.</span> <span class="toc-text">Activated (positive) and negative anchors</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Proposal-module"><span class="toc-number">2.3.</span> <span class="toc-text">Proposal module</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Loss-Function"><span class="toc-number">2.4.</span> <span class="toc-text">Loss Function</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Confidence-score-regression"><span class="toc-number">2.4.1.</span> <span class="toc-text">Confidence score regression</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Bounding-box-regression"><span class="toc-number">2.4.2.</span> <span class="toc-text">Bounding box regression</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#RPN-module"><span class="toc-number">2.5.</span> <span class="toc-text">RPN module</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#RPN-solver"><span class="toc-number">2.6.</span> <span class="toc-text">RPN solver</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#RPN-Overfit-small-data"><span class="toc-number">2.7.</span> <span class="toc-text">RPN - Overfit small data</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Faster-R-CNN"><span class="toc-number">3.</span> <span class="toc-text">Faster R-CNN</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#RoI-Align"><span class="toc-number">3.1.</span> <span class="toc-text">RoI Align</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Faster-R-CNN-1"><span class="toc-number">3.2.</span> <span class="toc-text">Faster R-CNN</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Overfit-small-data"><span class="toc-number">3.3.</span> <span class="toc-text">Overfit small data</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Inference"><span class="toc-number">3.3.1.</span> <span class="toc-text">Inference</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Train-a-net"><span class="toc-number">3.4.</span> <span class="toc-text">Train a net</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Inference-1"><span class="toc-number">3.4.1.</span> <span class="toc-text">Inference</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Evaluation"><span class="toc-number">3.5.</span> <span class="toc-text">Evaluation</span></a></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="images/avatar/image.jpeg"></div><div class="author-info__name text-center">Jiajun (Jayson) Bao</div><div class="author-info__description text-center">Graduate Student @LTIatCMU. B.S. 20' @EECSatMI</div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="archives"><span class="pull-left">Articles</span><span class="pull-right">7</span></a><a class="author-info-articles__tags article-meta" href="tags"><span class="pull-left">Tags</span><span class="pull-right">7</span></a><a class="author-info-articles__categories article-meta" href="categories"><span class="pull-left">Categories</span><span class="pull-right">3</span></a></div></div></div><div id="content-outer"><div class="no-bg" id="top-container"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">Jiajun (Jayson) Bao's</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">Home</a><a class="site-page" href="/about">About</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span><span class="pull-right"></span></div><div id="post-info"><div id="post-title">Two-Stage Object Detector - Faster R-CNN</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-05-07</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="categories/Projects/">Projects</a><i class="fa fa-angle-right" aria-hidden="true"></i><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="categories/Projects/Deep-Learning/">Deep Learning</a><i class="fa fa-angle-right" aria-hidden="true"></i><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="categories/Projects/Deep-Learning/Computer-Vision/">Computer Vision</a></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><p>In this project, we will implement a <strong>two-stage</strong> object detector, based on <a href="https://arxiv.org/pdf/1506.01497.pdf" target="_blank" rel="noopener">Faster R-CNN</a>, which consists of two modules, Region Proposal Networks (RPN) and Fast R-CNN. We will later use it to train a model that can detect objects on novel images and evaluate the detection accuracy using the classic metric mean Average Precision (<a href="https://github.com/Cartucho/mAP" target="_blank" rel="noopener">mAP</a>). </p>
<blockquote>
<p>This project is my solution to the course assignments of <a href="http://web.eecs.umich.edu/~justincj/teaching/eecs498/schedule.htmls" target="_blank" rel="noopener">EECS 498-007 / 598-005</a>, taught by Prof. Justin Johnson. “Turning in the same code as another student”, “looking at another student’s code while writing your own”, or “looking at solutions to similar assignments that we find online” is a violation of <a href="https://elc.engin.umich.edu/policies-and-interpretations/" target="_blank" rel="noopener">the Honor Code</a>.</p>
</blockquote>
<h1 id="Getting-Started"><a href="#Getting-Started" class="headerlink" title="Getting Started"></a>Getting Started</h1><h2 id="Install-starter-code"><a href="#Install-starter-code" class="headerlink" title="Install starter code"></a>Install starter code</h2><p><a href="https://github.com/deepvision-class/starter-code" target="_blank" rel="noopener"><code>coutils</code> package</a> provides utility solvers and tools needed for later sections.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!pip install git+https://github.com/deepvision-<span class="class"><span class="keyword">class</span>/<span class="title">starter</span>-<span class="title">code</span></span></span><br></pre></td></tr></table></figure>

<h2 id="Setup-code"><a href="#Setup-code" class="headerlink" title="Setup code"></a>Setup code</h2><p>Run some setup code for this notebook: Import some useful packages and increase the default figure size.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!pip install git+https://github.com/deepvision-<span class="class"><span class="keyword">class</span>/<span class="title">starter</span>-<span class="title">code</span></span></span><br></pre></td></tr></table></figure>

<h2 id="Setup-code-1"><a href="#Setup-code-1" class="headerlink" title="Setup code"></a>Setup code</h2><p>Run some setup code for this notebook: Import some useful packages and increase the default figure size.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> coutils</span><br><span class="line"><span class="keyword">from</span> coutils <span class="keyword">import</span> extract_drive_file_id, register_colab_notebooks, \</span><br><span class="line">                    fix_random_seed, rel_error</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> copy</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> shutil</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="comment"># for plotting</span></span><br><span class="line">plt.rcParams[<span class="string">'figure.figsize'</span>] = (<span class="number">10.0</span>, <span class="number">8.0</span>) <span class="comment"># set default size of plots</span></span><br><span class="line">plt.rcParams[<span class="string">'image.interpolation'</span>] = <span class="string">'nearest'</span></span><br><span class="line">plt.rcParams[<span class="string">'image.cmap'</span>] = <span class="string">'gray'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># data type and device for torch.tensor</span></span><br><span class="line">to_float = &#123;<span class="string">'dtype'</span>: torch.float, <span class="string">'device'</span>: <span class="string">'cpu'</span>&#125;</span><br><span class="line">to_float_cuda = &#123;<span class="string">'dtype'</span>: torch.float, <span class="string">'device'</span>: <span class="string">'cuda'</span>&#125;</span><br><span class="line">to_double = &#123;<span class="string">'dtype'</span>: torch.double, <span class="string">'device'</span>: <span class="string">'cpu'</span>&#125;</span><br><span class="line">to_double_cuda = &#123;<span class="string">'dtype'</span>: torch.double, <span class="string">'device'</span>: <span class="string">'cuda'</span>&#125;</span><br><span class="line">to_long = &#123;<span class="string">'dtype'</span>: torch.long, <span class="string">'device'</span>: <span class="string">'cpu'</span>&#125;</span><br><span class="line">to_long_cuda = &#123;<span class="string">'dtype'</span>: torch.long, <span class="string">'device'</span>: <span class="string">'cuda'</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># for mAP evaluation</span></span><br><span class="line">!rm -rf mAP</span><br><span class="line">!git clone https://github.com/Cartucho/mAP.git</span><br></pre></td></tr></table></figure>

<p>We will use GPUs to accelerate our computation in this notebook. Run the following to make sure GPUs are enabled:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> torch.cuda.is_available:</span><br><span class="line">  print(<span class="string">'Good to go!'</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">  print(<span class="string">'Please set GPU via Edit -&gt; Notebook Settings.'</span>)</span><br></pre></td></tr></table></figure>
<h2 id="Import-functions-from-previous-notebook"><a href="#Import-functions-from-previous-notebook" class="headerlink" title="Import functions from previous notebook"></a>Import functions from previous notebook</h2><p>Like what we have seen in the previous post, this project will re-use some pieces of code that we implemented in the previous post.<br>If you have not yet read the post on YOLO, you can find it here.</p>
<h2 id="Load-PASCAL-VOC-2007-data"><a href="#Load-PASCAL-VOC-2007-data" class="headerlink" title="Load PASCAL VOC 2007 data"></a>Load PASCAL VOC 2007 data</h2><p>As in the previous notebook, we will use the PASCAL VOC 2007 dataset to train our object detection system.</p>
<p>As in the previous notebook, we will subsample the dataset and wrap it in a DataLoader that can form minibatches for us.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># uncomment below to use the mirror link if the original link is broken</span></span><br><span class="line"><span class="comment"># !wget http://pjreddie.com/media/files/VOCtrainval_06-Nov-2007.tar</span></span><br><span class="line">train_dataset = get_pascal_voc2007_data(<span class="string">'/content'</span>, <span class="string">'train'</span>)</span><br><span class="line">val_dataset = get_pascal_voc2007_data(<span class="string">'/content'</span>, <span class="string">'val'</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">class_to_idx = &#123;<span class="string">'aeroplane'</span>:<span class="number">0</span>, <span class="string">'bicycle'</span>:<span class="number">1</span>, <span class="string">'bird'</span>:<span class="number">2</span>, <span class="string">'boat'</span>:<span class="number">3</span>, <span class="string">'bottle'</span>:<span class="number">4</span>,</span><br><span class="line">                <span class="string">'bus'</span>:<span class="number">5</span>, <span class="string">'car'</span>:<span class="number">6</span>, <span class="string">'cat'</span>:<span class="number">7</span>, <span class="string">'chair'</span>:<span class="number">8</span>, <span class="string">'cow'</span>:<span class="number">9</span>, <span class="string">'diningtable'</span>:<span class="number">10</span>,</span><br><span class="line">                <span class="string">'dog'</span>:<span class="number">11</span>, <span class="string">'horse'</span>:<span class="number">12</span>, <span class="string">'motorbike'</span>:<span class="number">13</span>, <span class="string">'person'</span>:<span class="number">14</span>, <span class="string">'pottedplant'</span>:<span class="number">15</span>,</span><br><span class="line">                <span class="string">'sheep'</span>:<span class="number">16</span>, <span class="string">'sofa'</span>:<span class="number">17</span>, <span class="string">'train'</span>:<span class="number">18</span>, <span class="string">'tvmonitor'</span>:<span class="number">19</span></span><br><span class="line">&#125;</span><br><span class="line">idx_to_class = &#123;i:c <span class="keyword">for</span> c, i <span class="keyword">in</span> class_to_idx.items()&#125;</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">train_dataset = torch.utils.data.Subset(train_dataset, torch.arange(<span class="number">0</span>, <span class="number">2500</span>)) <span class="comment"># use 2500 samples for training</span></span><br><span class="line">train_loader = pascal_voc2007_loader(train_dataset, <span class="number">10</span>)</span><br><span class="line">val_loader = pascal_voc2007_loader(val_dataset, <span class="number">10</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">train_loader_iter = iter(train_loader)</span><br><span class="line">img, ann, _, _, _ = train_loader_iter.next()</span><br><span class="line"></span><br><span class="line">print(<span class="string">'Resized train images shape: '</span>, img[<span class="number">0</span>].shape)</span><br><span class="line">print(<span class="string">'Padded annotation tensor shape: '</span>, ann[<span class="number">0</span>].shape)</span><br><span class="line">print(ann[<span class="number">0</span>])</span><br><span class="line">print(<span class="string">'Each row in the annotation tensor indicates (x_tl, y_tl, x_br, y_br, class).'</span>)</span><br><span class="line">print(<span class="string">'Padded with bounding boxes (-1, -1, -1, -1, -1) to enable batch loading. (You may need to run a few times to see the paddings)'</span>)</span><br></pre></td></tr></table></figure>

<pre><code>Resized train images shape:  torch.Size([3, 224, 224])
Padded annotation tensor shape:  torch.Size([6, 5])
tensor([[156.,  97., 351., 270.,   6.],
        [ -1.,  -1.,  -1.,  -1.,  -1.],
        [ -1.,  -1.,  -1.,  -1.,  -1.],
        [ -1.,  -1.,  -1.,  -1.,  -1.],
        [ -1.,  -1.,  -1.,  -1.,  -1.],
        [ -1.,  -1.,  -1.,  -1.,  -1.]])
Each row in the annotation tensor indicates (x_tl, y_tl, x_br, y_br, class).
Padded with bounding boxes (-1, -1, -1, -1, -1) to enable batch loading. (You may need to run a few times to see the paddings)</code></pre><h2 id="Visualize-PASCAL-VOC-2007"><a href="#Visualize-PASCAL-VOC-2007" class="headerlink" title="Visualize PASCAL VOC 2007"></a>Visualize PASCAL VOC 2007</h2><p>Sample a couple of images and GT boxes.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># default examples for visualization</span></span><br><span class="line">fix_random_seed(<span class="number">0</span>)</span><br><span class="line">batch_size = <span class="number">3</span></span><br><span class="line">sampled_idx = torch.linspace(<span class="number">0</span>, len(train_dataset)<span class="number">-1</span>, steps=batch_size).long()</span><br><span class="line"></span><br><span class="line"><span class="comment"># get the size of each image first</span></span><br><span class="line">h_list = []</span><br><span class="line">w_list = []</span><br><span class="line">img_list = [] <span class="comment"># list of images</span></span><br><span class="line">MAX_NUM_BBOX = <span class="number">40</span></span><br><span class="line">box_list = torch.LongTensor(batch_size, MAX_NUM_BBOX, <span class="number">5</span>).fill_(<span class="number">-1</span>) <span class="comment"># PADDED GT boxes</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> idx, i <span class="keyword">in</span> enumerate(sampled_idx):</span><br><span class="line">  <span class="comment"># hack to get the original image so we don't have to load from local again...</span></span><br><span class="line">  img, ann = train_dataset.__getitem__(i)</span><br><span class="line">  img_list.append(img)</span><br><span class="line"></span><br><span class="line">  all_bbox = ann[<span class="string">'annotation'</span>][<span class="string">'object'</span>]</span><br><span class="line">  <span class="keyword">if</span> type(all_bbox) == dict:</span><br><span class="line">    all_bbox = [all_bbox]</span><br><span class="line">  <span class="keyword">for</span> bbox_idx, one_bbox <span class="keyword">in</span> enumerate(all_bbox):</span><br><span class="line">    bbox = one_bbox[<span class="string">'bndbox'</span>]</span><br><span class="line">    obj_cls = one_bbox[<span class="string">'name'</span>]</span><br><span class="line">    box_list[idx][bbox_idx] = torch.LongTensor([int(bbox[<span class="string">'xmin'</span>]), int(bbox[<span class="string">'ymin'</span>]),</span><br><span class="line">      int(bbox[<span class="string">'xmax'</span>]), int(bbox[<span class="string">'ymax'</span>]), class_to_idx[obj_cls]])</span><br><span class="line"></span><br><span class="line">  <span class="comment"># get sizes</span></span><br><span class="line">  img = np.array(img)</span><br><span class="line">  w_list.append(img.shape[<span class="number">1</span>])</span><br><span class="line">  h_list.append(img.shape[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">w_list = torch.tensor(w_list, **to_float_cuda)</span><br><span class="line">h_list = torch.tensor(h_list, **to_float_cuda)</span><br><span class="line">box_list = torch.tensor(box_list, **to_float_cuda)</span><br><span class="line">resized_box_list = coord_trans(box_list, w_list, h_list, mode=<span class="string">'p2a'</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># visualize GT boxes</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(img_list)):</span><br><span class="line">  valid_box = sum([<span class="number">1</span> <span class="keyword">if</span> j != <span class="number">-1</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> j <span class="keyword">in</span> box_list[i][:, <span class="number">0</span>]])</span><br><span class="line">  data_visualizer(img_list[i], idx_to_class, box_list[i][:valid_box])</span><br></pre></td></tr></table></figure>


<p><img src="faster-rcnn/images/output_19_0.png" alt=""> <img src="faster-rcnn/images/output_19_1.png" alt=""> <img src="faster-rcnn/images/output_19_2.png" alt=""></p>
<h1 id="Region-Proposal-Networks-RPN"><a href="#Region-Proposal-Networks-RPN" class="headerlink" title="Region Proposal Networks (RPN)"></a>Region Proposal Networks (RPN)</h1><p>The first stage in a Faster R-CNN object detector is the <em>Region Proposal Network (RPN)</em>. The RPN classifies a set of anchors as either containing an object or not, and also regresses from the position of the anchor box to a region proposal.</p>
<p>The RPN is very similar to the single-stage detector we built in the previous notebook, except that it will not predict classification scores. We can therefore reuse many of the functions from the previous notebook in order to implement the RPN.</p>
<h2 id="Anchor"><a href="#Anchor" class="headerlink" title="Anchor"></a>Anchor</h2><p>We will use the exact same set of anchors as in the single-stage detector from the previous notebook.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Declare variables for anchor priors, a Ax2 Tensor where A is the number of anchors.</span></span><br><span class="line"><span class="comment"># Hand-picked, same as our two-stage detector.</span></span><br><span class="line">anchor_list = torch.tensor([[<span class="number">1</span>, <span class="number">1</span>], [<span class="number">2</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">4</span>], [<span class="number">5</span>, <span class="number">5</span>], [<span class="number">2</span>, <span class="number">3</span>], [<span class="number">3</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">5</span>], [<span class="number">5</span>, <span class="number">3</span>]], **to_float_cuda)</span><br><span class="line">print(anchor_list.shape)</span><br></pre></td></tr></table></figure>


<h2 id="Activated-positive-and-negative-anchors"><a href="#Activated-positive-and-negative-anchors" class="headerlink" title="Activated (positive) and negative anchors"></a>Activated (positive) and negative anchors</h2><p>When training the RPN, we compare the anchor boxes with the ground-truth boxes in order to determine a ground-truth label for the anchor boxes – should each anchor predict object or background?</p>
<p>We assign a positive label to two kinds of anchors:</p>
<p>(i) the anchor/anchors with the highest Intersection-overUnion (IoU) overlap with a ground-truth box, or</p>
<p>(ii) an anchor that has an IoU overlap higher than 0.7 with any ground-truth box. Note that a single ground-truth box may assign positive labels to multiple anchors.</p>
<p>Usually the second condition is sufficient to determine the positive samples; but we still adopt the first condition for the reason that in some rare cases the second condition may find no positive sample.</p>
<p>We assign a negative label to a non-positive anchor if its IoU ratio is lower than 0.3 for all ground-truth boxes. Anchors that are neither positive nor negative do not contribute to the training objective</p>
<p>We can implement anchor generation and matching to ground-truth by reusing the <code>GenerateGrid</code>, <code>GenerateAnchor</code>, <code>IoU</code>, and <code>ReferenceOnActivatedAnchors</code> functions from the previous notebook.</p>
<p>Run the following to check the implementation from A5-1 (with your IoU function). We should see errors on the order of 1e-7 or less.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">fix_random_seed(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">grid_list = GenerateGrid(w_list.shape[<span class="number">0</span>])</span><br><span class="line">anc_list = GenerateAnchor(anchor_list, grid_list)</span><br><span class="line">iou_mat = IoU(anc_list, resized_box_list)</span><br><span class="line">activated_anc_ind, negative_anc_ind, GT_conf_scores, GT_offsets, GT_class, \</span><br><span class="line">  activated_anc_coord, negative_anc_coord = ReferenceOnActivatedAnchors(anc_list, resized_box_list, grid_list, iou_mat)</span><br><span class="line"></span><br><span class="line">expected_GT_conf_scores = torch.tensor([<span class="number">0.74538743</span>, <span class="number">0.72793430</span>, <span class="number">0.71128041</span>, <span class="number">0.70029843</span>,</span><br><span class="line">                                        <span class="number">0.75670898</span>, <span class="number">0.76044953</span>, <span class="number">0.37116671</span>, <span class="number">0.37116671</span>], **to_float_cuda)</span><br><span class="line">expected_GT_offsets = torch.tensor([[ <span class="number">0.01633334</span>,  <span class="number">0.11911901</span>, <span class="number">-0.09431065</span>,  <span class="number">0.19244696</span>],</span><br><span class="line">                                    [<span class="number">-0.03675002</span>,  <span class="number">0.09324861</span>, <span class="number">-0.00250307</span>,  <span class="number">0.25213102</span>],</span><br><span class="line">                                    [<span class="number">-0.03675002</span>, <span class="number">-0.15675139</span>, <span class="number">-0.00250307</span>,  <span class="number">0.25213102</span>],</span><br><span class="line">                                    [<span class="number">-0.02940002</span>,  <span class="number">0.07459889</span>, <span class="number">-0.22564663</span>,  <span class="number">0.02898745</span>],</span><br><span class="line">                                    [ <span class="number">0.11879997</span>,  <span class="number">0.03208542</span>,  <span class="number">0.20863886</span>, <span class="number">-0.07974572</span>],</span><br><span class="line">                                    [<span class="number">-0.08120003</span>,  <span class="number">0.03208542</span>,  <span class="number">0.20863886</span>, <span class="number">-0.07974572</span>],</span><br><span class="line">                                    [ <span class="number">0.07699990</span>,  <span class="number">0.28533328</span>, <span class="number">-0.03459148</span>, <span class="number">-0.86750042</span>],</span><br><span class="line">                                    [ <span class="number">0.07699990</span>, <span class="number">-0.21466672</span>, <span class="number">-0.03459148</span>, <span class="number">-0.86750042</span>]], **to_float_cuda)</span><br><span class="line">expected_GT_class = torch.tensor([ <span class="number">6</span>,  <span class="number">7</span>,  <span class="number">7</span>,  <span class="number">7</span>, <span class="number">19</span>, <span class="number">19</span>,  <span class="number">6</span>,  <span class="number">6</span>], **to_long_cuda)</span><br><span class="line">print(<span class="string">'conf scores error: '</span>, rel_error(GT_conf_scores, expected_GT_conf_scores))</span><br><span class="line">print(<span class="string">'offsets error: '</span>, rel_error(GT_offsets, expected_GT_offsets))</span><br><span class="line">print(<span class="string">'class prob error: '</span>, rel_error(GT_class, expected_GT_class))</span><br></pre></td></tr></table></figure>

<pre><code>number of pos proposals:  8
conf scores error:  0.0
offsets error:  7.441442448907765e-07
class prob error:  0</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># visualize the activated anchors</span></span><br><span class="line">anc_per_img = torch.prod(torch.tensor(anc_list.shape[<span class="number">1</span>:<span class="number">-1</span>]))</span><br><span class="line"></span><br><span class="line">print(<span class="string">'*'</span>*<span class="number">80</span>)</span><br><span class="line">print(<span class="string">'Activated (positive) anchors:'</span>)</span><br><span class="line"><span class="keyword">for</span> img, bbox, idx <span class="keyword">in</span> zip(img_list, box_list, torch.arange(box_list.shape[<span class="number">0</span>])):</span><br><span class="line">  anc_ind_in_img = (activated_anc_ind &gt;= idx * anc_per_img) &amp; (activated_anc_ind &lt; (idx+<span class="number">1</span>) * anc_per_img)</span><br><span class="line">  print(<span class="string">'&#123;&#125; activated anchors!'</span>.format(torch.sum(anc_ind_in_img)))</span><br><span class="line">  data_visualizer(img, idx_to_class, bbox[:, :<span class="number">4</span>], coord_trans(activated_anc_coord[anc_ind_in_img], w_list[idx], h_list[idx]))</span><br><span class="line"></span><br><span class="line">print(<span class="string">'*'</span>*<span class="number">80</span>)</span><br><span class="line">print(<span class="string">'Negative anchors:'</span>)</span><br><span class="line"><span class="keyword">for</span> img, bbox, idx <span class="keyword">in</span> zip(img_list, box_list, torch.arange(box_list.shape[<span class="number">0</span>])):</span><br><span class="line">  anc_ind_in_img = (negative_anc_ind &gt;= idx * anc_per_img) &amp; (negative_anc_ind &lt; (idx+<span class="number">1</span>) * anc_per_img)</span><br><span class="line">  print(<span class="string">'&#123;&#125; negative anchors!'</span>.format(torch.sum(anc_ind_in_img)))</span><br><span class="line">  data_visualizer(img, idx_to_class, bbox[:, :<span class="number">4</span>], coord_trans(negative_anc_coord[anc_ind_in_img], w_list[idx], h_list[idx]))</span><br></pre></td></tr></table></figure>

<pre><code>********************************************************************************
Activated (positive) anchors:
1 activated anchors!</code></pre><p><img src="faster-rcnn/images/output_26_1.png" alt=""></p>
<pre><code>5 activated anchors!</code></pre><p><img src="faster-rcnn/images/output_26_3.png" alt=""></p>
<pre><code>2 activated anchors!</code></pre><p><img src="faster-rcnn/images/output_26_5.png" alt=""></p>
<pre><code>********************************************************************************
Negative anchors:
3 negative anchors!</code></pre><p><img src="faster-rcnn/images/output_26_7.png" alt=""></p>
<pre><code>2 negative anchors!</code></pre><p><img src="faster-rcnn/images/output_26_9.png" alt=""></p>
<pre><code>3 negative anchors!</code></pre><p><img src="faster-rcnn/images/output_26_11.png" alt=""></p>
<h2 id="Proposal-module"><a href="#Proposal-module" class="headerlink" title="Proposal module"></a>Proposal module</h2><p>Similar to the Prediction Networks in A5-1, but for RPN we only need to predict the object proposal scores (from the <em>cls</em> layer) and bounding box offsets (from the <em>reg</em> layer), all of which are class-agnostic.</p>
<p><img src="https://miro.medium.com/max/918/1*wB3ctS9WGNmw6pP_kjLjgg.png" alt="pred_scores2"></p>
<p>Note that here $k$ is essentially $A$. Image credit: Ren et al, “Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks”, NeurIPS 2015, <a href="https://arxiv.org/abs/1506.01497" target="_blank" rel="noopener">https://arxiv.org/abs/1506.01497</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ProposalModule</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_dim, hidden_dim=<span class="number">256</span>, num_anchors=<span class="number">9</span>, drop_ratio=<span class="number">0.3</span>)</span>:</span></span><br><span class="line">    super().__init__()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span>(num_anchors != <span class="number">0</span>)</span><br><span class="line">    self.num_anchors = num_anchors</span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment">#  Define the region proposal layer - a sequential module with a 3x3    #</span></span><br><span class="line">    <span class="comment"># conv layer, followed by a Dropout (p=drop_ratio), a Leaky ReLU and         #</span></span><br><span class="line">    <span class="comment"># a 1x1 conv.                                                                #</span></span><br><span class="line">    <span class="comment"># HINT: The output should be of shape Bx(Ax6)x7x7, where A=self.num_anchors. #</span></span><br><span class="line">    <span class="comment">#       Determine the padding of the 3x3 conv layer given the output dim.    #</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># Replace "pass" statement with your code</span></span><br><span class="line">    self.model = nn.Sequential(</span><br><span class="line">          nn.Conv2d(in_dim, hidden_dim, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>),</span><br><span class="line">          nn.Dropout(drop_ratio),</span><br><span class="line">          nn.LeakyReLU(),</span><br><span class="line">          nn.Conv2d(hidden_dim, <span class="number">6</span> * self.num_anchors, kernel_size=<span class="number">1</span>)</span><br><span class="line">        )</span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment">#                                                         #</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">_extract_anchor_data</span><span class="params">(self, anchor_data, anchor_idx)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - anchor_data: Tensor of shape (B, A, D, H, W) giving a vector of length</span></span><br><span class="line"><span class="string">      D for each of A anchors at each point in an H x W grid.</span></span><br><span class="line"><span class="string">    - anchor_idx: int64 Tensor of shape (M,) giving anchor indices to extract</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    - extracted_anchors: Tensor of shape (M, D) giving anchor data for each</span></span><br><span class="line"><span class="string">      of the anchors specified by anchor_idx.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    B, A, D, H, W = anchor_data.shape</span><br><span class="line">    anchor_data = anchor_data.permute(<span class="number">0</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">2</span>).contiguous().view(<span class="number">-1</span>, D)</span><br><span class="line">    extracted_anchors = anchor_data[anchor_idx]</span><br><span class="line">    <span class="keyword">return</span> extracted_anchors</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, features, pos_anchor_coord=None, \</span></span></span><br><span class="line"><span class="function"><span class="params">              pos_anchor_idx=None, neg_anchor_idx=None)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Run the forward pass of the proposal module.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - features: Tensor of shape (B, in_dim, H', W') giving features from the</span></span><br><span class="line"><span class="string">      backbone network.</span></span><br><span class="line"><span class="string">    - pos_anchor_coord: Tensor of shape (M, 4) giving the coordinates of</span></span><br><span class="line"><span class="string">      positive anchors. Anchors are specified as (x_tl, y_tl, x_br, y_br) with</span></span><br><span class="line"><span class="string">      the coordinates of the top-left corner (x_tl, y_tl) and bottom-right</span></span><br><span class="line"><span class="string">      corner (x_br, y_br). During inference this is None.</span></span><br><span class="line"><span class="string">    - pos_anchor_idx: int64 Tensor of shape (M,) giving the indices of positive</span></span><br><span class="line"><span class="string">      anchors. During inference this is None.</span></span><br><span class="line"><span class="string">    - neg_anchor_idx: int64 Tensor of shape (M,) giving the indicdes of negative</span></span><br><span class="line"><span class="string">      anchors. During inference this is None.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    The outputs from this module are different during training and inference.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    During training, pos_anchor_coord, pos_anchor_idx, and neg_anchor_idx are</span></span><br><span class="line"><span class="string">    all provided, and we only output predictions for the positive and negative</span></span><br><span class="line"><span class="string">    anchors. During inference, these are all None and we must output predictions</span></span><br><span class="line"><span class="string">    for all anchors.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Outputs (during training):</span></span><br><span class="line"><span class="string">    - conf_scores: Tensor of shape (2M, 2) giving the classification scores</span></span><br><span class="line"><span class="string">      (object vs background) for each of the M positive and M negative anchors.</span></span><br><span class="line"><span class="string">    - offsets: Tensor of shape (M, 4) giving predicted transforms for the</span></span><br><span class="line"><span class="string">      M positive anchors.</span></span><br><span class="line"><span class="string">    - proposals: Tensor of shape (M, 4) giving predicted region proposals for</span></span><br><span class="line"><span class="string">      the M positive anchors.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Outputs (during inference):</span></span><br><span class="line"><span class="string">    - conf_scores: Tensor of shape (B, A, 2, H', W') giving the predicted</span></span><br><span class="line"><span class="string">      classification scores (object vs background) for all anchors</span></span><br><span class="line"><span class="string">    - offsets: Tensor of shape (B, A, 4, H', W') giving the predicted transforms</span></span><br><span class="line"><span class="string">      for all anchors</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">if</span> pos_anchor_coord <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> pos_anchor_idx <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> neg_anchor_idx <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">      mode = <span class="string">'eval'</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      mode = <span class="string">'train'</span></span><br><span class="line">    conf_scores, offsets, proposals = <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">    <span class="comment">############################################################################</span></span><br><span class="line">    <span class="comment">#  Predict classification scores (object vs background) and transforms#</span></span><br><span class="line">    <span class="comment"># for all anchors. During inference, simply output predictions for all     #</span></span><br><span class="line">    <span class="comment"># anchors. During training, extract the predictions for only the positive  #</span></span><br><span class="line">    <span class="comment"># and negative anchors as described above, and also apply the transforms to#</span></span><br><span class="line">    <span class="comment"># the positive anchors to compute the coordinates of the region proposals. #</span></span><br><span class="line">    <span class="comment">#                                                                          #</span></span><br><span class="line">    <span class="comment"># HINT: We can extract information about specific proposals using the     #</span></span><br><span class="line">    <span class="comment"># provided helper function self._extract_anchor_data.                      #</span></span><br><span class="line">    <span class="comment"># HINT: We can compute proposal coordinates using the GenerateProposal    #</span></span><br><span class="line">    <span class="comment"># function from the previous notebook.                                     #</span></span><br><span class="line">    <span class="comment">############################################################################</span></span><br><span class="line">    <span class="comment"># Replace "pass" statement with your code</span></span><br><span class="line">    out = self.model(features)  <span class="comment"># (B, Ax6, 7, 7)</span></span><br><span class="line">    A = self.num_anchors</span><br><span class="line">    B, _, H_amap, W_amap = out.shape</span><br><span class="line">    out = out.view(B, A, <span class="number">6</span>, H_amap, W_amap)</span><br><span class="line">    conf_scores = out[:, :, :<span class="number">2</span>, :, :]</span><br><span class="line">    offsets = out[:, :, <span class="number">2</span>:, :, :]</span><br><span class="line">    <span class="keyword">if</span> mode == <span class="string">'train'</span>:</span><br><span class="line">      grid = GenerateGrid(B, w_amap=W_amap, h_amap = H_amap)  <span class="comment"># (B, H_amap, W_amap, 2)</span></span><br><span class="line">      anchors = GenerateAnchor(anchor_list.to(**to_float_cuda), grid)  <span class="comment"># (B, A, H_amap, W_amap, 4)</span></span><br><span class="line">      offsets_perm = offsets.permute(<span class="number">0</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">2</span>)</span><br><span class="line">      proposals = GenerateProposal(anchors, offsets_perm, <span class="string">'FasterRCNN'</span>)  <span class="comment"># (B, A, H_amap, W_amap, 4)</span></span><br><span class="line">      proposals = proposals.permute(<span class="number">0</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">      proposals = self._extract_anchor_data(proposals, pos_anchor_idx)</span><br><span class="line">      anchor_idx = torch.cat((pos_anchor_idx, neg_anchor_idx), dim=<span class="number">0</span>)</span><br><span class="line">      conf_scores = self._extract_anchor_data(conf_scores, anchor_idx)</span><br><span class="line">      <span class="comment"># modification to the offsets should be after proposals</span></span><br><span class="line">      offsets = self._extract_anchor_data(offsets, pos_anchor_idx)</span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment">#                                                         #</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="keyword">if</span> mode == <span class="string">'train'</span>:</span><br><span class="line">      <span class="keyword">return</span> conf_scores, offsets, proposals</span><br><span class="line">    <span class="keyword">elif</span> mode == <span class="string">'eval'</span>:</span><br><span class="line">      <span class="keyword">return</span> conf_scores, offsets</span><br></pre></td></tr></table></figure>

<p>Run the following to check your implementation. We should see errors on the order of 1e-7 or less.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># sanity check</span></span><br><span class="line">fix_random_seed(<span class="number">0</span>)</span><br><span class="line">prop_module = ProposalModule(<span class="number">1280</span>, drop_ratio=<span class="number">0</span>).to(**to_float_cuda)</span><br><span class="line">features = torch.linspace(<span class="number">-10.</span>, <span class="number">10.</span>, steps=<span class="number">3</span>*<span class="number">1280</span>*<span class="number">7</span>*<span class="number">7</span>, **to_float_cuda).view(<span class="number">3</span>, <span class="number">1280</span>, <span class="number">7</span>, <span class="number">7</span>)</span><br><span class="line">conf_scores, offsets, proposals = prop_module(features, activated_anc_coord, \</span><br><span class="line">              pos_anchor_idx=activated_anc_ind, neg_anchor_idx=negative_anc_ind)</span><br><span class="line"></span><br><span class="line">expected_conf_scores = torch.tensor([[<span class="number">-0.50843990</span>,  <span class="number">2.62025023</span>],</span><br><span class="line">                                     [<span class="number">-0.55775326</span>, <span class="number">-0.29983672</span>],</span><br><span class="line">                                     [<span class="number">-0.55796617</span>, <span class="number">-0.30000290</span>],</span><br><span class="line">                                     [ <span class="number">0.17819080</span>, <span class="number">-0.42211828</span>],</span><br><span class="line">                                     [<span class="number">-0.51439995</span>, <span class="number">-0.47708601</span>],</span><br><span class="line">                                     [<span class="number">-0.51439744</span>, <span class="number">-0.47703803</span>],</span><br><span class="line">                                     [ <span class="number">0.63225138</span>,  <span class="number">2.71269488</span>],</span><br><span class="line">                                     [ <span class="number">0.63224381</span>,  <span class="number">2.71290708</span>]], **to_float_cuda)</span><br><span class="line">expected_offsets = torch.tensor([[ <span class="number">1.62754285</span>,  <span class="number">1.35253453</span>, <span class="number">-1.85451591</span>, <span class="number">-1.77882397</span>],</span><br><span class="line">                                 [<span class="number">-0.33651856</span>, <span class="number">-0.14402901</span>, <span class="number">-0.07458937</span>, <span class="number">-0.27201492</span>],</span><br><span class="line">                                 [<span class="number">-0.33671042</span>, <span class="number">-0.14398587</span>, <span class="number">-0.07479107</span>, <span class="number">-0.27199429</span>],</span><br><span class="line">                                 [ <span class="number">0.06847382</span>,  <span class="number">0.21062726</span>,  <span class="number">0.09334904</span>, <span class="number">-0.02446130</span>],</span><br><span class="line">                                 [ <span class="number">0.16506940</span>, <span class="number">-0.30296192</span>,  <span class="number">0.29626080</span>,  <span class="number">0.32173073</span>],</span><br><span class="line">                                 [ <span class="number">0.16507357</span>, <span class="number">-0.30302414</span>,  <span class="number">0.29625297</span>,  <span class="number">0.32169008</span>],</span><br><span class="line">                                 [ <span class="number">1.59992146</span>, <span class="number">-0.75236654</span>,  <span class="number">1.66449440</span>,  <span class="number">2.05138564</span>],</span><br><span class="line">                                 [ <span class="number">1.60008609</span>, <span class="number">-0.75249159</span>,  <span class="number">1.66474164</span>,  <span class="number">2.05162382</span>]], **to_float_cuda)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'conf scores error: '</span>, rel_error(conf_scores[:<span class="number">8</span>], expected_conf_scores))</span><br><span class="line">print(<span class="string">'offsets error: '</span>, rel_error(offsets, expected_offsets))</span><br></pre></td></tr></table></figure>

<pre><code>conf scores error:  0.0
offsets error:  0.0</code></pre><h2 id="Loss-Function"><a href="#Loss-Function" class="headerlink" title="Loss Function"></a>Loss Function</h2><p>The confidence score regression loss is for both activated/negative anchors while the bounding box regression loss loss is for activated anchors only.</p>
<h3 id="Confidence-score-regression"><a href="#Confidence-score-regression" class="headerlink" title="Confidence score regression"></a>Confidence score regression</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ConfScoreRegression</span><span class="params">(conf_scores, batch_size)</span>:</span></span><br><span class="line">  <span class="string">"""</span></span><br><span class="line"><span class="string">  Binary cross-entropy loss</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Inputs:</span></span><br><span class="line"><span class="string">  - conf_scores: Predicted confidence scores, of shape (2M, 2). Assume that the</span></span><br><span class="line"><span class="string">    first M are positive samples, and the last M are negative samples.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Outputs:</span></span><br><span class="line"><span class="string">  - conf_score_loss: Torch scalar</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line">  <span class="comment"># the target conf_scores for positive samples are ones and negative are zeros</span></span><br><span class="line">  M = conf_scores.shape[<span class="number">0</span>] // <span class="number">2</span></span><br><span class="line">  GT_conf_scores = torch.zeros_like(conf_scores)</span><br><span class="line">  GT_conf_scores[:M, <span class="number">0</span>] = <span class="number">1.</span></span><br><span class="line">  GT_conf_scores[M:, <span class="number">1</span>] = <span class="number">1.</span></span><br><span class="line"></span><br><span class="line">  conf_score_loss = F.binary_cross_entropy_with_logits(conf_scores, GT_conf_scores, \</span><br><span class="line">                                     reduction=<span class="string">'sum'</span>) * <span class="number">1.</span> / batch_size</span><br><span class="line">  <span class="keyword">return</span> conf_score_loss</span><br></pre></td></tr></table></figure>

<h3 id="Bounding-box-regression"><a href="#Bounding-box-regression" class="headerlink" title="Bounding box regression"></a>Bounding box regression</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">BboxRegression</span><span class="params">(offsets, GT_offsets, batch_size)</span>:</span></span><br><span class="line">  <span class="string">""""</span></span><br><span class="line"><span class="string">  Use SmoothL1 loss as in Faster R-CNN</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Inputs:</span></span><br><span class="line"><span class="string">  - offsets: Predicted box offsets, of shape (M, 4)</span></span><br><span class="line"><span class="string">  - GT_offsets: GT box offsets, of shape (M, 4)</span></span><br><span class="line"><span class="string">  </span></span><br><span class="line"><span class="string">  Outputs:</span></span><br><span class="line"><span class="string">  - bbox_reg_loss: Torch scalar</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line">  bbox_reg_loss = F.smooth_l1_loss(offsets, GT_offsets, reduction=<span class="string">'sum'</span>) * <span class="number">1.</span> / batch_size</span><br><span class="line">  <span class="keyword">return</span> bbox_reg_loss</span><br></pre></td></tr></table></figure>

<p>Run the following to check your implementation. We should see errors on the order of 1e-7 or less.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">conf_loss = ConfScoreRegression(conf_scores, features.shape[<span class="number">0</span>])</span><br><span class="line">reg_loss = BboxRegression(offsets, GT_offsets, features.shape[<span class="number">0</span>])</span><br><span class="line">print(<span class="string">'conf loss: &#123;:.4f&#125;, reg loss: &#123;:.4f&#125;'</span>.format(conf_loss, reg_loss))</span><br><span class="line"></span><br><span class="line">loss_all = torch.tensor([conf_loss.data, reg_loss.data], **to_float_cuda)</span><br><span class="line">expected_loss = torch.tensor([<span class="number">8.55673981</span>, <span class="number">5.10593748</span>], **to_float_cuda)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'loss error: '</span>, rel_error(loss_all, expected_loss))</span><br></pre></td></tr></table></figure>

<pre><code>conf loss: 8.5567, reg loss: 5.1059
loss error:  0.0</code></pre><h2 id="RPN-module"><a href="#RPN-module" class="headerlink" title="RPN module"></a>RPN module</h2><p>Implement Region Proposal Network. Should resemble the <code>SingleStageDetector</code> module from A5-1, but without the class prediction.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RPN</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">    super().__init__()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># READ ONLY</span></span><br><span class="line">    self.anchor_list = torch.tensor([[<span class="number">1</span>, <span class="number">1</span>], [<span class="number">2</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">4</span>], [<span class="number">5</span>, <span class="number">5</span>], [<span class="number">2</span>, <span class="number">3</span>], [<span class="number">3</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">5</span>], [<span class="number">5</span>, <span class="number">3</span>]])</span><br><span class="line">    self.feat_extractor = FeatureExtractor()</span><br><span class="line">    self.prop_module = ProposalModule(<span class="number">1280</span>, num_anchors=self.anchor_list.shape[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, images, bboxes, output_mode=<span class="string">'loss'</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Training-time forward pass for the Region Proposal Network.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - images: Tensor of shape (B, 3, 224, 224) giving input images</span></span><br><span class="line"><span class="string">    - bboxes: Tensor of ground-truth bounding boxes, returned from the DataLoader</span></span><br><span class="line"><span class="string">    - output_mode: One of 'loss' or 'all' that determines what is returned:</span></span><br><span class="line"><span class="string">      If output_mode is 'loss' then the output is:</span></span><br><span class="line"><span class="string">      - total_loss: Torch scalar giving the total RPN loss for the minibatch</span></span><br><span class="line"><span class="string">      If output_mode is 'all' then the output is:</span></span><br><span class="line"><span class="string">      - total_loss: Torch scalar giving the total RPN loss for the minibatch</span></span><br><span class="line"><span class="string">      - pos_conf_scores: Tensor of shape (M, 1) giving the object classification</span></span><br><span class="line"><span class="string">        scores (object vs background) for the positive anchors</span></span><br><span class="line"><span class="string">      - proposals: Tensor of shape (M, 4) giving the coordiantes of the region</span></span><br><span class="line"><span class="string">        proposals for the positive anchors</span></span><br><span class="line"><span class="string">      - features: Tensor of features computed from the backbone network</span></span><br><span class="line"><span class="string">      - GT_class: Tensor of shape (M,) giving the ground-truth category label</span></span><br><span class="line"><span class="string">        for the positive anchors.</span></span><br><span class="line"><span class="string">      - pos_anchor_idx: Tensor of shape (M,) giving indices of positive anchors</span></span><br><span class="line"><span class="string">      - neg_anchor_idx: Tensor of shape (M,) giving indices of negative anchors</span></span><br><span class="line"><span class="string">      - anc_per_image: Torch scalar giving the number of anchors per image.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Outputs: See output_mode</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    HINT: The function ReferenceOnActivatedAnchors from the previous notebook</span></span><br><span class="line"><span class="string">    can compute many of these outputs -- we should study it in detail:</span></span><br><span class="line"><span class="string">    - pos_anchor_idx (also called activated_anc_ind)</span></span><br><span class="line"><span class="string">    - neg_anchor_idx (also called negative_anc_ind)</span></span><br><span class="line"><span class="string">    - GT_class</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># weights to multiply to each loss term</span></span><br><span class="line">    w_conf = <span class="number">1</span> <span class="comment"># for conf_scores</span></span><br><span class="line">    w_reg = <span class="number">5</span> <span class="comment"># for offsets</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span> output_mode <span class="keyword">in</span> (<span class="string">'loss'</span>, <span class="string">'all'</span>), <span class="string">'invalid output mode!'</span></span><br><span class="line">    total_loss = <span class="literal">None</span></span><br><span class="line">    conf_scores, proposals, features, GT_class, pos_anchor_idx, anc_per_img = \</span><br><span class="line">      <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment">#  Implement the forward pass of RPN.                                   #</span></span><br><span class="line">    <span class="comment"># A few key steps are outlined as follows:                                   #</span></span><br><span class="line">    <span class="comment"># i) Image feature extraction,                                               #</span></span><br><span class="line">    <span class="comment"># ii) Grid and anchor generation,                                            #</span></span><br><span class="line">    <span class="comment"># iii) Compute IoU between anchors and GT boxes and then determine activated/#</span></span><br><span class="line">    <span class="comment">#      negative anchors, and GT_conf_scores, GT_offsets, GT_class,           #</span></span><br><span class="line">    <span class="comment"># iv) Compute conf_scores, offsets, proposals through the region proposal    #</span></span><br><span class="line">    <span class="comment">#     module                                                                 #</span></span><br><span class="line">    <span class="comment"># v) Compute the total_loss for RPN which is formulated as:                  #</span></span><br><span class="line">    <span class="comment">#    total_loss = w_conf * conf_loss + w_reg * reg_loss,                     #</span></span><br><span class="line">    <span class="comment">#    where conf_loss is determined by ConfScoreRegression, w_reg by          #</span></span><br><span class="line">    <span class="comment">#    BboxRegression. Note that RPN does not predict any class info.          #</span></span><br><span class="line">    <span class="comment">#    We have written this part for we which you've already practiced earlier#</span></span><br><span class="line">    <span class="comment"># HINT: Do not apply thresholding nor NMS on the proposals during training   #</span></span><br><span class="line">    <span class="comment">#       as positive/negative anchors have been explicitly targeted.          #</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># Replace "pass" statement with your code</span></span><br><span class="line"></span><br><span class="line">    batch_size, _, _, _ = images.shape</span><br><span class="line">    features = self.feat_extractor(images)  <span class="comment"># (N, 1280, 7, 7)</span></span><br><span class="line">    grid = GenerateGrid(batch_size=batch_size)  <span class="comment"># (B, H', W', 2)</span></span><br><span class="line">    anchors = GenerateAnchor(anc=self.anchor_list.to(**to_float_cuda), grid=grid)  <span class="comment"># (B, A, H', W', 4)</span></span><br><span class="line">    iou_mat = IoU(anchors, bboxes)  <span class="comment"># (B, A*H'*W', N)</span></span><br><span class="line">    activated_anc_ind, negative_anc_ind, GT_conf_scores, GT_offsets, GT_class, activated_anc_coord, negative_anc_coord = ReferenceOnActivatedAnchors(anchors, bboxes, grid, iou_mat)</span><br><span class="line"></span><br><span class="line">    conf_scores, offsets, proposals = self.prop_module(features, pos_anchor_coord=activated_anc_coord, pos_anchor_idx=activated_anc_ind, neg_anchor_idx=negative_anc_ind)</span><br><span class="line"></span><br><span class="line">    anc_per_img = torch.prod(torch.tensor(anchors.shape[<span class="number">1</span>:<span class="number">-1</span>]))</span><br><span class="line">    pos_anchor_idx = activated_anc_ind</span><br><span class="line">    conf_loss = ConfScoreRegression(conf_scores, batch_size)</span><br><span class="line">    reg_loss = BboxRegression(offsets, GT_offsets, batch_size)</span><br><span class="line"></span><br><span class="line">    total_loss = w_conf * conf_loss + w_reg * reg_loss</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> output_mode == <span class="string">'loss'</span>:</span><br><span class="line">      <span class="keyword">return</span> total_loss</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      <span class="keyword">return</span> total_loss, conf_scores, proposals, features, GT_class, pos_anchor_idx, anc_per_img</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">inference</span><span class="params">(self, images, thresh=<span class="number">0.5</span>, nms_thresh=<span class="number">0.7</span>, mode=<span class="string">'RPN'</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Inference-time forward pass for the Region Proposal Network.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - images: Tensor of shape (B, 3, H, W) giving input images</span></span><br><span class="line"><span class="string">    - thresh: Threshold value on confidence scores. Proposals with a predicted</span></span><br><span class="line"><span class="string">      object probability above thresh should be kept. HINT: We can convert the</span></span><br><span class="line"><span class="string">      object score to an object probability using a sigmoid nonlinearity.</span></span><br><span class="line"><span class="string">    - nms_thresh: IoU threshold for non-maximum suppression</span></span><br><span class="line"><span class="string">    - mode: One of 'RPN' or 'FasterRCNN' to determine the outputs.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    The region proposal network can output a variable number of region proposals</span></span><br><span class="line"><span class="string">    per input image. We assume that the input image images[i] gives rise to</span></span><br><span class="line"><span class="string">    P_i final propsals after thresholding and NMS.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    NOTE: NMS is performed independently per-image!</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Outputs:</span></span><br><span class="line"><span class="string">    - final_proposals: List of length B, where final_proposals[i] is a Tensor</span></span><br><span class="line"><span class="string">      of shape (P_i, 4) giving the coordinates of the predicted region proposals</span></span><br><span class="line"><span class="string">      for the input image images[i].</span></span><br><span class="line"><span class="string">    - final_conf_probs: List of length B, where final_conf_probs[i] is a</span></span><br><span class="line"><span class="string">      Tensor of shape (P_i,) giving the predicted object probabilities for each</span></span><br><span class="line"><span class="string">      predicted region proposal for images[i]. Note that these are</span></span><br><span class="line"><span class="string">      *probabilities*, not scores, so they should be between 0 and 1.</span></span><br><span class="line"><span class="string">    - features: Tensor of shape (B, D, H', W') giving the image features</span></span><br><span class="line"><span class="string">      predicted by the backbone network for each element of images.</span></span><br><span class="line"><span class="string">      If mode is "RPN" then this is a dummy list of zeros instead.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">assert</span> mode <span class="keyword">in</span> (<span class="string">'RPN'</span>, <span class="string">'FasterRCNN'</span>), <span class="string">'invalid inference mode!'</span></span><br><span class="line"></span><br><span class="line">    features, final_conf_probs, final_proposals = <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment">#  Predicting the RPN proposal coordinates `final_proposals` and        #</span></span><br><span class="line">    <span class="comment"># confidence scores `final_conf_probs`.                                     #</span></span><br><span class="line">    <span class="comment"># The overall steps are similar to the forward pass but now we do not need  #</span></span><br><span class="line">    <span class="comment"># to decide the activated nor negative anchors.                              #</span></span><br><span class="line">    <span class="comment"># HINT: Threshold the conf_scores based on the threshold value `thresh`.     #</span></span><br><span class="line">    <span class="comment"># Then, apply NMS to the filtered proposals given the threshold `nms_thresh`.#</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># Replace "pass" statement with your code</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">      final_proposals, final_conf_probs = list(), list()</span><br><span class="line">      batch_size, _, _, _ = images.shape</span><br><span class="line">      features = self.feat_extractor(images)  <span class="comment"># (N, 1280, 7, 7)</span></span><br><span class="line">      grid = GenerateGrid(batch_size=batch_size)  <span class="comment"># (B, H', W', 2)</span></span><br><span class="line">      anchors = GenerateAnchor(anc=self.anchor_list.to(**to_float_cuda), grid=grid)  <span class="comment"># (B, A, H', W', 4)</span></span><br><span class="line"></span><br><span class="line">      conf_scores, offsets = self.prop_module(features)  <span class="comment"># (B, A, 2, H_amap, W_amap), (B, A, 4, H_amap, W_amap)</span></span><br><span class="line">      offsets = offsets.permute(<span class="number">0</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">2</span>)</span><br><span class="line">      proposals = GenerateProposal(anchors, offsets, method=<span class="string">'FasterRCNN'</span>)  <span class="comment"># (B, A, H_amap, W_amap, 4)</span></span><br><span class="line">    </span><br><span class="line">      conf_probs = torch.sigmoid(conf_scores[:, :, <span class="number">0</span>, :, :])</span><br><span class="line">      mask = conf_probs &gt; thresh  <span class="comment"># (B, A, 2, H_amap, W_amap)</span></span><br><span class="line">    </span><br><span class="line">      <span class="keyword">for</span> b <span class="keyword">in</span> range(batch_size):</span><br><span class="line">        proposal = proposals[b][mask[b]]  <span class="comment"># (?,)</span></span><br><span class="line">        conf_prob = conf_probs[b][mask[b]]</span><br><span class="line">        keep = torchvision.ops.nms(proposal, conf_prob, nms_thresh)  <span class="comment"># (M,)</span></span><br><span class="line">        final_proposals.append(proposal[keep])</span><br><span class="line">        final_conf_probs.append(conf_prob[keep].unsqueeze(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> mode == <span class="string">'RPN'</span>:</span><br><span class="line">      features = [torch.zeros_like(i) <span class="keyword">for</span> i <span class="keyword">in</span> final_conf_probs] <span class="comment"># dummy class</span></span><br><span class="line">    <span class="keyword">return</span> final_proposals, final_conf_probs, features</span><br></pre></td></tr></table></figure>

<h2 id="RPN-solver"><a href="#RPN-solver" class="headerlink" title="RPN solver"></a>RPN solver</h2><p>In Faster R-CNN, the RPN is trained jointly with the second-stage network. However, to test our RPN implementation, we will first train just the RPN; this is basically a class-agnostic single-stage detector, that only classifies regions as object or background.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">RPNSolver = DetectionSolver <span class="comment"># the same solver as in YOLO</span></span><br></pre></td></tr></table></figure>

<h2 id="RPN-Overfit-small-data"><a href="#RPN-Overfit-small-data" class="headerlink" title="RPN - Overfit small data"></a>RPN - Overfit small data</h2><p>First we will overfit the RPN on a small subset of the PASCAL VOC 2007 dataset. After training we should see a loss around or less than 3.0.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># monitor the training loss</span></span><br><span class="line">num_sample = <span class="number">10</span></span><br><span class="line">small_dataset = torch.utils.data.Subset(train_dataset, torch.linspace(<span class="number">0</span>, len(train_dataset)<span class="number">-1</span>, steps=num_sample).long())</span><br><span class="line">small_train_loader = pascal_voc2007_loader(small_dataset, <span class="number">10</span>) <span class="comment"># a new loader</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> lr <span class="keyword">in</span> [<span class="number">1e-3</span>]:</span><br><span class="line">  print(<span class="string">'lr: '</span>, lr)</span><br><span class="line">  rpn = RPN()</span><br><span class="line">  RPNSolver(rpn, small_train_loader, learning_rate=lr, num_epochs=<span class="number">200</span>)</span><br><span class="line">```                      </span><br><span class="line">    (Iter <span class="number">0</span> / <span class="number">1</span>)</span><br><span class="line">    (Epoch <span class="number">1</span> / <span class="number">200</span>) loss: <span class="number">59.7316</span> time per epoch: <span class="number">0.2</span>s</span><br><span class="line">    number of pos proposals:  <span class="number">50</span></span><br><span class="line">    (Iter <span class="number">0</span> / <span class="number">1</span>)</span><br><span class="line">    (Epoch <span class="number">2</span> / <span class="number">200</span>) loss: <span class="number">25.3708</span> time per epoch: <span class="number">0.1</span>s</span><br><span class="line">    number of pos proposals:  <span class="number">50</span></span><br><span class="line">    (Iter <span class="number">0</span> / <span class="number">1</span>)</span><br><span class="line">    (Epoch <span class="number">3</span> / <span class="number">200</span>) loss: <span class="number">20.5340</span> time per epoch: <span class="number">0.1</span>s</span><br><span class="line">    number of pos proposals:  <span class="number">50</span></span><br><span class="line">    ......</span><br><span class="line">    (Iter <span class="number">0</span> / <span class="number">1</span>)</span><br><span class="line">    (Epoch <span class="number">193</span> / <span class="number">200</span>) loss: <span class="number">2.5435</span> time per epoch: <span class="number">0.1</span>s</span><br><span class="line">    number of pos proposals:  <span class="number">50</span></span><br><span class="line">    (Iter <span class="number">0</span> / <span class="number">1</span>)</span><br><span class="line">    (Epoch <span class="number">194</span> / <span class="number">200</span>) loss: <span class="number">2.8138</span> time per epoch: <span class="number">0.2</span>s</span><br><span class="line">    number of pos proposals:  <span class="number">50</span></span><br><span class="line">    (Iter <span class="number">0</span> / <span class="number">1</span>)</span><br><span class="line">    (Epoch <span class="number">195</span> / <span class="number">200</span>) loss: <span class="number">2.3827</span> time per epoch: <span class="number">0.1</span>s</span><br><span class="line">    number of pos proposals:  <span class="number">50</span></span><br><span class="line">    (Iter <span class="number">0</span> / <span class="number">1</span>)</span><br><span class="line">    (Epoch <span class="number">196</span> / <span class="number">200</span>) loss: <span class="number">2.1514</span> time per epoch: <span class="number">0.1</span>s</span><br><span class="line">    number of pos proposals:  <span class="number">50</span></span><br><span class="line">    (Iter <span class="number">0</span> / <span class="number">1</span>)</span><br><span class="line">    (Epoch <span class="number">197</span> / <span class="number">200</span>) loss: <span class="number">2.8080</span> time per epoch: <span class="number">0.1</span>s</span><br><span class="line">    number of pos proposals:  <span class="number">50</span></span><br><span class="line">    (Iter <span class="number">0</span> / <span class="number">1</span>)</span><br><span class="line">    (Epoch <span class="number">198</span> / <span class="number">200</span>) loss: <span class="number">2.2378</span> time per epoch: <span class="number">0.1</span>s</span><br><span class="line">    number of pos proposals:  <span class="number">50</span></span><br><span class="line">    (Iter <span class="number">0</span> / <span class="number">1</span>)</span><br><span class="line">    (Epoch <span class="number">199</span> / <span class="number">200</span>) loss: <span class="number">2.7982</span> time per epoch: <span class="number">0.1</span>s</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">![](faster-rcnn/images/output_43_5.png)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## RPN - Inference</span></span><br><span class="line">We will now visualize the predicted boxes <span class="keyword">from</span> the RPN that we overfit to a small training sample. We will reuse the `DetectionInference` function <span class="keyword">from</span> the previous notebook.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">```python</span><br><span class="line">RPNInference = DetectionInference</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># visualize the output from the overfitted model on small dataset</span></span><br><span class="line"><span class="comment"># the bounding boxes should be really accurate</span></span><br><span class="line"><span class="comment"># ignore the dummy object class (in blue) as RPN does not output class!</span></span><br><span class="line">RPNInference(rpn, small_train_loader, small_dataset, idx_to_class, thresh=<span class="number">0.8</span>, nms_thresh=<span class="number">0.3</span>)</span><br></pre></td></tr></table></figure>


<p><img src="faster-rcnn/images/output_46_0.png" alt=""></p>
<p><img src="faster-rcnn/images/output_46_1.png" alt=""></p>
<p><img src="faster-rcnn/images/output_46_2.png" alt=""></p>
<p><img src="faster-rcnn/images/output_46_3.png" alt=""></p>
<p><img src="faster-rcnn/images/output_46_4.png" alt=""></p>
<p><img src="faster-rcnn/images/output_46_5.png" alt=""></p>
<p><img src="faster-rcnn/images/output_46_6.png" alt=""></p>
<p><img src="faster-rcnn/images/output_46_7.png" alt=""></p>
<p><img src="faster-rcnn/images/output_46_8.png" alt=""></p>
<p><img src="faster-rcnn/images/output_46_9.png" alt=""></p>
<pre><code>Total inference time: 3.0s</code></pre><h1 id="Faster-R-CNN"><a href="#Faster-R-CNN" class="headerlink" title="Faster R-CNN"></a>Faster R-CNN</h1><p>We have implemented the first half of Faster R-CNN, i.e., RPN, which is class-agnostic. Here, we briefly describe the second half Fast R-CNN.</p>
<p>Given the proposals or region of interests (RoI) from RPN, we warp each region from CNN activation map to a fixed size 2x2 by using <a href="https://arxiv.org/pdf/1703.06870.pdf" target="_blank" rel="noopener">RoI Align</a>. Essentially, the RoI feature is determined by bilinear interpolation over the CNN activation map. We meanpool the RoI feature over the spatial dimension (2x2).</p>
<p>Finally, we classify the meanpooled RoI feature into class probabilities.</p>
<p>For simplicity, our two-stage detector here differs from a full Faster R-CNN system in a few aspects.</p>
<ol>
<li>In a full implementation, the second stage of the network would predict a second set of offsets to transform the region proposal into a final predicted object bounding box. However we omit this for simplicity.</li>
<li>In a full implementation, the second stage of the network should be able to reject negative boxes – in other words, if we want to predict C different object categories then the final classification layer of the second stage would predict a distribution over C+1 categories, with an extra one for background. We omit this, as it requires extra bookeeping in the second stage about which proposals are positive / negative; so for simplicity our second stage will only predict a distribution over C categories, and we will assume that the RPN has filtered out all background regions.</li>
</ol>
<h2 id="RoI-Align"><a href="#RoI-Align" class="headerlink" title="RoI Align"></a>RoI Align</h2><p>We will use the <code>roi_align</code> function from <code>torchvision</code>. Usage see <a href="https://pytorch.org/docs/stable/torchvision/ops.html#torchvision.ops.roi_align" target="_blank" rel="noopener">https://pytorch.org/docs/stable/torchvision/ops.html#torchvision.ops.roi_align</a></p>
<h2 id="Faster-R-CNN-1"><a href="#Faster-R-CNN-1" class="headerlink" title="Faster R-CNN"></a>Faster R-CNN</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TwoStageDetector</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_dim=<span class="number">1280</span>, hidden_dim=<span class="number">256</span>, num_classes=<span class="number">20</span>, \</span></span></span><br><span class="line"><span class="function"><span class="params">               roi_output_w=<span class="number">2</span>, roi_output_h=<span class="number">2</span>, drop_ratio=<span class="number">0.3</span>)</span>:</span></span><br><span class="line">    super().__init__()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span>(num_classes != <span class="number">0</span>)</span><br><span class="line">    self.num_classes = num_classes</span><br><span class="line">    self.roi_output_w, self.roi_output_h = roi_output_w, roi_output_h</span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment">#  Declare your RPN and the region classification layer (in Fast R-CNN).#</span></span><br><span class="line">    <span class="comment"># The region classification layer is a sequential module with a Linear layer,#</span></span><br><span class="line">    <span class="comment"># followed by a Dropout (p=drop_ratio), a ReLU nonlinearity and another      #</span></span><br><span class="line">    <span class="comment"># Linear layer that predicts classification scores for each proposal.        #</span></span><br><span class="line">    <span class="comment"># HINT: The dimension of the two Linear layers are in_dim -&gt; hidden_dim and  #</span></span><br><span class="line">    <span class="comment"># hidden_dim -&gt; num_classes.                                                 #</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># Replace "pass" statement with your code</span></span><br><span class="line">    self.rpn = RPN()</span><br><span class="line">    self.model = nn.Sequential(</span><br><span class="line">          nn.Linear(in_dim, hidden_dim),</span><br><span class="line">          nn.Dropout(drop_ratio),</span><br><span class="line">          nn.LeakyReLU(),</span><br><span class="line">          nn.Linear(hidden_dim, num_classes)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, images, bboxes)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Training-time forward pass for our two-stage Faster R-CNN detector.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - images: Tensor of shape (B, 3, H, W) giving input images</span></span><br><span class="line"><span class="string">    - bboxes: Tensor of shape (B, N, 5) giving ground-truth bounding boxes</span></span><br><span class="line"><span class="string">      and category labels, from the dataloader.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Outputs:</span></span><br><span class="line"><span class="string">    - total_loss: Torch scalar giving the overall training loss.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    total_loss = <span class="literal">None</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment">#  Implement the forward pass of TwoStageDetector.                      #</span></span><br><span class="line">    <span class="comment"># A few key steps are outlined as follows:                                   #</span></span><br><span class="line">    <span class="comment"># i) RPN, including image feature extraction, grid/anchor/proposal           #</span></span><br><span class="line">    <span class="comment">#       generation, activated and negative anchors determination.            #</span></span><br><span class="line">    <span class="comment"># ii) Perform RoI Align on proposals and meanpool the feature in the spatial #</span></span><br><span class="line">    <span class="comment">#     dimension.                                                             #</span></span><br><span class="line">    <span class="comment"># iii) Pass the RoI feature through the region classification layer which    #</span></span><br><span class="line">    <span class="comment">#      gives the class probilities.                                          #</span></span><br><span class="line">    <span class="comment"># iv) Compute class_prob through the prediction network and compute the      #</span></span><br><span class="line">    <span class="comment">#     cross entropy loss (cls_loss) between the prediction class_prob and    #</span></span><br><span class="line">    <span class="comment">#      the reference GT_class. Hint: Use F.cross_entropy loss.               #</span></span><br><span class="line">    <span class="comment"># v) Compute the total_loss which is formulated as:                          #</span></span><br><span class="line">    <span class="comment">#    total_loss = rpn_loss + cls_loss.                                       #</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># Replace "pass" statement with your code</span></span><br><span class="line">    rpn_loss, conf_scores, proposals, features, GT_class, pos_anchor_idx, anc_per_img = self.rpn.forward(images, bboxes, output_mode=<span class="string">'all'</span>)</span><br><span class="line">    <span class="comment">#          (M, 1),     (M, 4),              (M,)       (M,)            (M,)</span></span><br><span class="line">    B, _, H, W = images.shape</span><br><span class="line">    M, _ = proposals.shape</span><br><span class="line">    idx = (pos_anchor_idx // anc_per_img).view(<span class="number">-1</span>, <span class="number">1</span>).to(images)</span><br><span class="line">    proposals = torch.cat((idx, proposals), <span class="number">1</span>)</span><br><span class="line">    roi = torchvision.ops.roi_align(features, proposals, (self.roi_output_h, self.roi_output_w))  <span class="comment"># (M, 1280, 7, 7)</span></span><br><span class="line">    mean_pool = nn.AvgPool2d(self.roi_output_h)  <span class="comment"># (M, 1280, 7, 7)</span></span><br><span class="line">    roi = mean_pool(roi).view(M, <span class="number">-1</span>)  <span class="comment"># (M, 1280)</span></span><br><span class="line"></span><br><span class="line">    class_prob = self.model(roi)</span><br><span class="line">    cls_loss = F.cross_entropy(class_prob, GT_class, reduction=<span class="string">'sum'</span>) * <span class="number">1.</span> / M</span><br><span class="line">    total_loss = rpn_loss + cls_loss</span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment">#                                                         #</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="keyword">return</span> total_loss</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">inference</span><span class="params">(self, images, thresh=<span class="number">0.5</span>, nms_thresh=<span class="number">0.7</span>)</span>:</span></span><br><span class="line">    <span class="string">""""</span></span><br><span class="line"><span class="string">    Inference-time forward pass for our two-stage Faster R-CNN detector</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - images: Tensor of shape (B, 3, H, W) giving input images</span></span><br><span class="line"><span class="string">    - thresh: Threshold value on NMS object probabilities</span></span><br><span class="line"><span class="string">    - nms_thresh: IoU threshold for NMS in the RPN</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    We can output a variable number of predicted boxes per input image.</span></span><br><span class="line"><span class="string">    In particular we assume that the input images[i] gives rise to P_i final</span></span><br><span class="line"><span class="string">    predicted boxes.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Outputs:</span></span><br><span class="line"><span class="string">    - final_proposals: List of length (B,) where final_proposals[i] is a Tensor</span></span><br><span class="line"><span class="string">      of shape (P_i, 4) giving the coordinates of the final predicted boxes for</span></span><br><span class="line"><span class="string">      the input images[i]</span></span><br><span class="line"><span class="string">    - final_conf_probs: List of length (B,) where final_conf_probs[i] is a</span></span><br><span class="line"><span class="string">      Tensor of shape (P_i,) giving the predicted probabilites that the boxes</span></span><br><span class="line"><span class="string">      in final_proposals[i] are objects (vs background)</span></span><br><span class="line"><span class="string">    - final_class: List of length (B,), where final_class[i] is an int64 Tensor</span></span><br><span class="line"><span class="string">      of shape (P_i,) giving the predicted category labels for each box in</span></span><br><span class="line"><span class="string">      final_proposals[i].</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    final_proposals, final_conf_probs, final_class = <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment">#  Predicting the final proposal coordinates `final_proposals`,        #</span></span><br><span class="line">    <span class="comment"># confidence scores `final_conf_probs`, and the class index `final_class`.  #</span></span><br><span class="line">    <span class="comment"># The overall steps are similar to the forward pass but now we do not need #</span></span><br><span class="line">    <span class="comment"># to decide the activated nor negative anchors.                             #</span></span><br><span class="line">    <span class="comment"># HINT: Use the RPN inference function to perform thresholding and NMS, and #</span></span><br><span class="line">    <span class="comment"># to compute final_proposals and final_conf_probs. Use the predicted class  #</span></span><br><span class="line">    <span class="comment"># probabilities from the second-stage network to compute final_class.       #</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># Replace "pass" statement with your code</span></span><br><span class="line">    final_class = []</span><br><span class="line">    B, _, H, W = images.shape</span><br><span class="line">    final_proposals, final_conf_probs, features = self.rpn.inference(images, thresh, nms_thresh, mode=<span class="string">'FasterRCNN'</span>)</span><br><span class="line">    <span class="comment"># (N, 4), (N,), (B, D, H, W)</span></span><br><span class="line">    <span class="keyword">for</span> b <span class="keyword">in</span> range(B):</span><br><span class="line">      proposal = final_proposals[b]</span><br><span class="line">      <span class="keyword">if</span> proposal.shape[<span class="number">0</span>] == <span class="number">0</span>:  <span class="comment"># if the list is empty</span></span><br><span class="line">        final_class.append(torch.tensor([]).to(images).view(<span class="number">-1</span>, <span class="number">1</span>))</span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line">      idx = torch.tensor([b]).to(images).view(<span class="number">-1</span>, <span class="number">1</span>).expand(proposal.shape[<span class="number">0</span>], <span class="number">-1</span>)</span><br><span class="line">      proposal_index = torch.cat((idx, proposal), dim=<span class="number">1</span>)</span><br><span class="line">      roi_feature = torchvision.ops.roi_align(features, proposal_index, (self.roi_output_h, self.roi_output_w))  <span class="comment"># (N, 3, H, W)</span></span><br><span class="line">      mean_pool = nn.AvgPool2d(self.roi_output_h)</span><br><span class="line">      roi_feature = mean_pool(roi_feature).view(proposal.shape[<span class="number">0</span>], <span class="number">-1</span>)  <span class="comment"># (N, 3, H, W)</span></span><br><span class="line">      class_probs = self.model(roi_feature)</span><br><span class="line">      _, class_index = torch.max(class_probs, <span class="number">1</span>)</span><br><span class="line">      final_class.append(class_index.reshape(<span class="number">-1</span>, <span class="number">1</span>))</span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment">#                                                         #</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="keyword">return</span> final_proposals, final_conf_probs, final_class</span><br></pre></td></tr></table></figure>

<h2 id="Overfit-small-data"><a href="#Overfit-small-data" class="headerlink" title="Overfit small data"></a>Overfit small data</h2><p>We will now overfit the full Faster R-CNN network on a small subset of the training data. After training we should see a final loss less than 4.0.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># monitor the training loss</span></span><br><span class="line"></span><br><span class="line">lr = <span class="number">1e-3</span></span><br><span class="line">detector = TwoStageDetector()</span><br><span class="line">DetectionSolver(detector, small_train_loader, learning_rate=lr, num_epochs=<span class="number">200</span>)</span><br></pre></td></tr></table></figure>

<pre><code>number of pos proposals:  50
(Iter 0 / 1)
(Epoch 0 / 200) loss: 28.0285 time per epoch: 0.2s
number of pos proposals:  50
(Iter 0 / 1)
(Epoch 1 / 200) loss: 50.7229 time per epoch: 0.2s
number of pos proposals:  50
(Iter 0 / 1)
(Epoch 2 / 200) loss: 27.0009 time per epoch: 0.2s
number of pos proposals:  50
(Iter 0 / 1)
(Epoch 3 / 200) loss: 22.0237 time per epoch: 0.1s
number of pos proposals:  50
(Iter 0 / 1)
(Epoch 4 / 200) loss: 20.1340 time per epoch: 0.1s
number of pos proposals:  50
(Iter 0 / 1)
(Epoch 5 / 200) loss: 19.0662 time per epoch: 0.2s
number of pos proposals:  50
......
(Iter 0 / 1)
(Epoch 196 / 200) loss: 3.0146 time per epoch: 0.2s
number of pos proposals:  50
(Iter 0 / 1)
(Epoch 197 / 200) loss: 3.5488 time per epoch: 0.1s
number of pos proposals:  50
(Iter 0 / 1)
(Epoch 198 / 200) loss: 3.9051 time per epoch: 0.2s
number of pos proposals:  50
(Iter 0 / 1)
(Epoch 199 / 200) loss: 3.3033 time per epoch: 0.1s</code></pre><p><img src="faster-rcnn/images/output_52_1.png" alt=""></p>
<h3 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># visualize the output from the overfitted model on small dataset</span></span><br><span class="line"><span class="comment"># the bounding boxes should be really accurate</span></span><br><span class="line">DetectionInference(detector, small_train_loader, small_dataset, idx_to_class, thresh=<span class="number">0.8</span>, nms_thresh=<span class="number">0.3</span>)</span><br></pre></td></tr></table></figure>


<p><img src="faster-rcnn/images/output_54_0.png" alt=""></p>
<p><img src="faster-rcnn/images/output_54_1.png" alt=""></p>
<p><img src="faster-rcnn/images/output_54_2.png" alt=""></p>
<p><img src="faster-rcnn/images/output_54_3.png" alt=""></p>
<p><img src="faster-rcnn/images/output_54_4.png" alt=""></p>
<p><img src="faster-rcnn/images/output_54_5.png" alt=""></p>
<p><img src="faster-rcnn/images/output_54_6.png" alt=""></p>
<p><img src="faster-rcnn/images/output_54_7.png" alt=""></p>
<p><img src="faster-rcnn/images/output_54_8.png" alt=""></p>
<p><img src="faster-rcnn/images/output_54_9.png" alt=""></p>
<pre><code>Total inference time: 3.0s</code></pre><h2 id="Train-a-net"><a href="#Train-a-net" class="headerlink" title="Train a net"></a>Train a net</h2><p>Now it’s time to train the full Faster R-CNN model on a larger subset of the the training data. We will train for 50 epochs; this should take about 35 minutes on a K80 GPU. We should see a total loss less than 3.0.</p>
<p>(Optional) If we train the model longer (e.g., 100 epochs), we should see a better mAP. But make sure we revert the code back for grading purposes.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># monitor the training loss</span></span><br><span class="line">train_loader = pascal_voc2007_loader(train_dataset, <span class="number">100</span>) <span class="comment"># a new loader</span></span><br><span class="line"></span><br><span class="line">num_epochs = <span class="number">50</span></span><br><span class="line">lr = <span class="number">5e-3</span></span><br><span class="line">frcnn_detector = TwoStageDetector()</span><br><span class="line">DetectionSolver(frcnn_detector, train_loader, learning_rate=lr, num_epochs=num_epochs)</span><br></pre></td></tr></table></figure>

<pre><code>number of pos proposals:  348
(Iter 0 / 25)
number of pos proposals:  410
(Iter 1 / 25)
number of pos proposals:  375
(Iter 2 / 25)
number of pos proposals:  362
(Iter 3 / 25)
number of pos proposals:  348
(Iter 4 / 25)
number of pos proposals:  316
......
(Iter 17 / 25)
number of pos proposals:  395
(Iter 18 / 25)
number of pos proposals:  386
(Iter 19 / 25)
number of pos proposals:  392
(Iter 20 / 25)
number of pos proposals:  405
(Iter 21 / 25)
number of pos proposals:  335
(Iter 22 / 25)
number of pos proposals:  359
(Iter 23 / 25)
number of pos proposals:  376
(Iter 24 / 25)
(Epoch 49 / 50) loss: 2.2144 time per epoch: 31.8s</code></pre><p><img src="faster-rcnn/images/output_56_1.png" alt=""></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># (optional) load/save checkpoint</span></span><br><span class="line"><span class="comment"># torch.save(frcnn_detector.state_dict(), 'frcnn_detector.pt') # uncomment to save your checkpoint</span></span><br><span class="line"><span class="comment"># frcnn_detector.load_state_dict(torch.load('frcnn_detector.pt')) # uncomment to load your previous checkpoint</span></span><br></pre></td></tr></table></figure>

<h3 id="Inference-1"><a href="#Inference-1" class="headerlink" title="Inference"></a>Inference</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># visualize the same output from the model trained on the entire training set</span></span><br><span class="line"><span class="comment"># some bounding boxes might not make sense</span></span><br><span class="line">DetectionInference(frcnn_detector, small_train_loader, small_dataset, idx_to_class)</span><br></pre></td></tr></table></figure>


<p><img src="faster-rcnn/images/output_59_0.png" alt=""></p>
<p><img src="faster-rcnn/images/output_59_1.png" alt=""></p>
<p><img src="faster-rcnn/images/output_59_2.png" alt=""></p>
<p><img src="faster-rcnn/images/output_59_3.png" alt=""></p>
<p><img src="faster-rcnn/images/output_59_4.png" alt=""></p>
<p><img src="faster-rcnn/images/output_59_5.png" alt=""></p>
<p><img src="faster-rcnn/images/output_59_6.png" alt=""></p>
<p><img src="faster-rcnn/images/output_59_7.png" alt=""></p>
<p><img src="faster-rcnn/images/output_59_8.png" alt=""></p>
<p><img src="faster-rcnn/images/output_59_9.png" alt=""></p>
<pre><code>Total inference time: 3.2s</code></pre><h2 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h2><p>Compute mean Average Precision (mAP). Introduction on mAP see lecture slides (p46-57): <a href="https://web.eecs.umich.edu/~justincj/slides/eecs498/498_FA2019_lecture15.pdf" target="_blank" rel="noopener">https://web.eecs.umich.edu/~justincj/slides/eecs498/498_FA2019_lecture15.pdf</a></p>
<p>Run the following to evaluate your detector on the PASCAL VOC validation set. We should see mAP at around 16% or above.</p>
<p>(Optional) If we train the model longer (e.g., 100 epochs), we should see a better mAP. But make sure we revert the code back for grading purposes.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">!rm -r mAP/input/*</span><br><span class="line">DetectionInference(frcnn_detector, val_loader, val_dataset, idx_to_class, output_dir=<span class="string">'mAP/input'</span>, thresh=<span class="number">0.8</span>, nms_thresh=<span class="number">0.3</span>)</span><br><span class="line"><span class="comment"># DetectionInference(frcnn_detector, train_loader, train_dataset, idx_to_class, output_dir='mAP/input', thresh=0.8, nms_thresh=0.3) # uncomment to see training mAP</span></span><br><span class="line">!cd mAP &amp;&amp; python main.py</span><br></pre></td></tr></table></figure>

<pre><code>000005.jpg: 5 GT bboxes and 6 proposals
000007.jpg: 1 GT bboxes and 1 proposals
000009.jpg: 4 GT bboxes and 6 proposals
000016.jpg: 1 GT bboxes and 2 proposals
......
009935.jpg: 11 GT bboxes and 20 proposals
009939.jpg: 2 GT bboxes and 5 proposals
009946.jpg: 4 GT bboxes and 2 proposals
009947.jpg: 2 GT bboxes and 2 proposals
009950.jpg: 2 GT bboxes and 1 proposals
009954.jpg: 2 GT bboxes and 4 proposals
009955.jpg: 2 GT bboxes and 3 proposals
009958.jpg: 5 GT bboxes and 17 proposals
Total inference time: 38.4s
20.66% = aeroplane AP 
11.68% = bicycle AP 
12.20% = bird AP 
6.03% = boat AP 
0.91% = bottle AP 
11.08% = bus AP 
16.76% = car AP 
40.72% = cat AP 
2.05% = chair AP 
10.96% = cow AP 
2.03% = diningtable AP 
27.07% = dog AP 
17.89% = horse AP 
23.65% = motorbike AP 
14.63% = person AP 
1.70% = pottedplant AP 
3.43% = sheep AP 
9.37% = sofa AP 
26.83% = train AP 
9.20% = tvmonitor AP 
mAP = 13.44%</code></pre></div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">Jiajun (Jayson) Bao</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="2020/05/07/two-stage-detector-faster-rcnn/">2020/05/07/two-stage-detector-faster-rcnn/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="tags/Object-Detection/">Object Detection</a><a class="post-meta__tags" href="tags/Faster-R-CNN/">Faster R-CNN</a></div><nav id="pagination"><div class="next-post pull-right"><a href="2020/05/03/hello-world/"><span>Under maintenance Until May 10</span><i class="fa fa-chevron-right"></i></a></div></nav><div id="gitalk-container"></div><script>var gitalk = new Gitalk({
  clientID: '7af9fc6163ec5f674641',
  clientSecret: '7c8309ce96d6e5dce003f52aaac9f6fd872173d7',
  repo: 'gitalk-storage',
  owner: 'JiajunBao',
  admin: '['JiajunBao']',
  id: md5(decodeURI(location.pathname)),
  language: 'en'
})
gitalk.render('gitalk-container')</script></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2017 - 2020 By Jiajun (Jayson) Bao</div><div class="framework-info"><span>Powered by : </span><a href="https://reactjs.org/" target="_blank" rel="noopener"><span>React</span></a><span class="footer-separator">|</span><span>Theme - </span><a href="https://www.cmu.edu/brand/brand-guidelines/visual-identity/colors.html" target="_blank" rel="noopener"><span>Carnegie Red</span></a></div><div class="footer_custom_text">hitokoto</div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.7.0"></script><script src="/js/fancybox.js?version=1.7.0"></script><script src="/js/sidebar.js?version=1.7.0"></script><script src="/js/copy.js?version=1.7.0"></script><script src="/js/fireworks.js?version=1.7.0"></script><script src="/js/transition.js?version=1.7.0"></script><script src="/js/scroll.js?version=1.7.0"></script><script src="/js/head.js?version=1.7.0"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css"><script src="/js/katex.js"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"log":false,"model":{"jsonPath":"/live2dw/assets/assets/hijiki.model.json"},"display":{"position":"left","width":150,"height":300},"mobile":{"show":true},"react":{"opacity":"1s"}});</script></body></html>